"""
Performance Tests - Issue #66

Tests database query performance to ensure p95 < 300ms target is met.
Validates that indexes are properly used and no N+1 query problems exist.

Run with: pytest tests/performance/test_query_performance.py -v --tb=short
"""

import pytest
import time
from sqlalchemy import text
from sqlalchemy.orm import Session
from typing import List, Dict
from uuid import uuid4
from datetime import datetime, timedelta

from app.db.models.application import Application
from app.db.models.job import Job
from app.db.models.candidate_profile import CandidateProfile
from app.db.models.company import Company, CompanyMember
from app.db.models.user import User
from app.services.application_service import ApplicationService
from app.services.candidate_search_service import CandidateSearchService
from app.schemas.candidate_profile import CandidateSearchFilters


# ===========================================================================
# Performance Test Configuration
# ===========================================================================

# Performance targets
TARGET_P95_MS = 300  # p95 query time must be under 300ms
TARGET_P99_MS = 500  # p99 query time must be under 500ms
SLOW_QUERY_THRESHOLD_MS = 500  # Queries over 500ms are considered slow

# Test data volumes (simulate production scale)
NUM_APPLICATIONS = 1000  # Test with 1K applications
NUM_JOBS = 100
NUM_CANDIDATES = 500
NUM_COMPANIES = 10


# ===========================================================================
# Fixtures
# ===========================================================================


@pytest.fixture(scope="module")
def performance_test_data(db: Session):
    """
    Create test data at production scale for performance testing.

    Simulates:
    - 10 companies
    - 100 jobs
    - 500 candidate profiles
    - 1000 applications
    """
    # Create companies
    companies = []
    for i in range(NUM_COMPANIES):
        company = Company(
            id=uuid4(),
            name=f"Test Company {i}",
            domain=f"company{i}.com",
            industry="Technology",
            company_size="50-200",
            description=f"Test company {i} description",
            subscription_tier="professional",
        )
        db.add(company)
        companies.append(company)

    # Create users (candidates and employers)
    users = []
    for i in range(NUM_CANDIDATES + 50):  # 500 candidates + 50 employers
        user = User(
            id=uuid4(),
            email=f"user{i}@test.com",
            hashed_password="test_hash",
            full_name=f"Test User {i}",
            user_type="candidate" if i < NUM_CANDIDATES else "employer",
            is_active=True,
            is_verified=True,
        )
        db.add(user)
        users.append(user)

    db.flush()

    # Create candidate profiles
    candidate_profiles = []
    for i in range(NUM_CANDIDATES):
        profile = CandidateProfile(
            id=uuid4(),
            user_id=users[i].id,
            headline=f"Software Engineer with {i % 10} years experience",
            bio=f"Experienced developer specializing in Python and JavaScript",
            current_title=f"Senior Developer" if i % 3 == 0 else "Software Engineer",
            current_location=f"San Francisco, CA" if i % 2 == 0 else "New York, NY",
            years_experience=i % 10,
            skills=["Python", "JavaScript", "React", "PostgreSQL"][:(i % 4) + 1],
            experience_level="senior" if i % 3 == 0 else "mid",
            min_salary=100000 + (i * 1000),
            max_salary=150000 + (i * 1000),
            visibility="public" if i % 2 == 0 else "private",
            open_to_work=True if i % 3 != 0 else False,
            preferred_location_type="remote" if i % 2 == 0 else "hybrid",
        )
        db.add(profile)
        candidate_profiles.append(profile)

    # Create jobs
    jobs = []
    for i in range(NUM_JOBS):
        company = companies[i % NUM_COMPANIES]
        job = Job(
            id=uuid4(),
            title=f"Software Engineer - Backend" if i % 2 == 0 else "Frontend Developer",
            company=company.name,
            company_id=company.id,
            location=f"San Francisco, CA" if i % 2 == 0 else "Remote",
            location_type="remote" if i % 3 == 0 else "onsite",
            job_type="full_time",
            salary_min=100000,
            salary_max=150000,
            description=f"Job description for position {i}",
            requirements=f"Requirements for position {i}",
            is_active=True if i % 5 != 0 else False,  # 80% active
            posted_date=datetime.utcnow() - timedelta(days=i),
        )
        db.add(job)
        jobs.append(job)

    db.flush()

    # Create applications
    applications = []
    for i in range(NUM_APPLICATIONS):
        job = jobs[i % NUM_JOBS]
        user = users[i % NUM_CANDIDATES]

        application = Application(
            id=uuid4(),
            user_id=user.id,
            job_id=job.id,
            status=["new", "reviewing", "phone_screen", "interview", "offer"][i % 5],
            fit_index=(i * 7) % 100,  # Distribute scores 0-100
            source="auto_apply" if i % 3 == 0 else "manual",
            is_auto_applied=True if i % 3 == 0 else False,
            applied_at=datetime.utcnow() - timedelta(hours=i),
            resume_url=f"https://storage.com/resumes/{i}.pdf",
            cover_letter_url=f"https://storage.com/letters/{i}.pdf",
        )
        db.add(application)
        applications.append(application)

    db.commit()

    yield {
        "companies": companies,
        "users": users,
        "jobs": jobs,
        "candidate_profiles": candidate_profiles,
        "applications": applications,
    }

    # Cleanup (optional - may want to keep data for manual testing)
    # db.query(Application).delete()
    # db.query(Job).delete()
    # db.query(CandidateProfile).delete()
    # db.query(User).delete()
    # db.query(Company).delete()
    # db.commit()


@pytest.fixture
def performance_tracker():
    """Track query execution times for percentile calculation"""
    times = []

    def track(func):
        start = time.perf_counter()
        result = func()
        elapsed_ms = (time.perf_counter() - start) * 1000
        times.append(elapsed_ms)
        return result, elapsed_ms

    def get_stats():
        if not times:
            return {}

        sorted_times = sorted(times)
        count = len(sorted_times)

        return {
            "count": count,
            "min": min(sorted_times),
            "max": max(sorted_times),
            "mean": sum(sorted_times) / count,
            "p50": sorted_times[int(count * 0.50)],
            "p95": sorted_times[int(count * 0.95)],
            "p99": sorted_times[int(count * 0.99)],
        }

    tracker = type('obj', (object,), {
        'track': track,
        'times': times,
        'get_stats': get_stats,
    })

    return tracker


# ===========================================================================
# Index Usage Tests
# ===========================================================================


def test_applications_job_stage_fit_index_used(db: Session, performance_test_data):
    """
    Verify idx_applications_job_stage_fit is used for employer ATS queries.

    This is the MOST CRITICAL query - used by employers to view applicants.
    """
    job_id = performance_test_data["jobs"][0].id

    # Execute EXPLAIN to verify index usage
    explain_query = text(f"""
        EXPLAIN (FORMAT JSON)
        SELECT * FROM applications
        WHERE job_id = :job_id AND status = 'reviewing'
        ORDER BY fit_index DESC
        LIMIT 20
    """)

    result = db.execute(explain_query, {"job_id": str(job_id)})
    plan = result.scalar()

    # Parse explain plan (JSON format)
    import json
    plan_data = json.loads(plan)
    plan_text = json.dumps(plan_data, indent=2)

    # Verify index is used
    assert "idx_applications_job_stage_fit" in plan_text or "Idx Scan" in plan_text, \
        f"Index not used! EXPLAIN plan:\n{plan_text}"

    print(f"‚úÖ Index idx_applications_job_stage_fit is being used")


def test_candidate_profiles_public_open_index_used(db: Session, performance_test_data):
    """
    Verify idx_candidate_profiles_public_open is used for candidate search.
    """
    explain_query = text("""
        EXPLAIN (FORMAT JSON)
        SELECT * FROM candidate_profiles
        WHERE visibility = 'public' AND open_to_work = TRUE
        LIMIT 20
    """)

    result = db.execute(explain_query)
    plan = result.scalar()

    import json
    plan_data = json.loads(plan)
    plan_text = json.dumps(plan_data, indent=2)

    assert "idx_candidate_profiles_public_open" in plan_text or "Idx Scan" in plan_text, \
        f"Index not used! EXPLAIN plan:\n{plan_text}"

    print(f"‚úÖ Index idx_candidate_profiles_public_open is being used")


# ===========================================================================
# Query Performance Tests
# ===========================================================================


def test_list_job_applications_performance(db: Session, performance_test_data, performance_tracker):
    """
    Test application listing performance (employer ATS view).

    Target: p95 < 300ms

    This is the most critical query for employers.
    """
    job_id = performance_test_data["jobs"][0].id
    service = ApplicationService(db)

    # Run query 20 times to calculate percentiles
    for i in range(20):
        result, elapsed_ms = performance_tracker.track(
            lambda: service.get_applications_for_job(
                job_id=job_id,
                status="reviewing",
                min_fit_index=70,
                sort_by="fit_index",
                order="desc",
                page=1,
                limit=20,
            )
        )

    stats = performance_tracker.get_stats()

    print(f"\nüìä Application Listing Performance:")
    print(f"   Runs: {stats['count']}")
    print(f"   Min: {stats['min']:.2f}ms")
    print(f"   Mean: {stats['mean']:.2f}ms")
    print(f"   p50: {stats['p50']:.2f}ms")
    print(f"   p95: {stats['p95']:.2f}ms")
    print(f"   p99: {stats['p99']:.2f}ms")
    print(f"   Max: {stats['max']:.2f}ms")

    # Assert p95 meets target
    assert stats['p95'] < TARGET_P95_MS, \
        f"p95 ({stats['p95']:.2f}ms) exceeds target ({TARGET_P95_MS}ms)"

    print(f"‚úÖ p95 ({stats['p95']:.2f}ms) meets target (<{TARGET_P95_MS}ms)")


def test_candidate_search_performance(db: Session, performance_test_data, performance_tracker):
    """
    Test candidate profile search performance.

    Target: p95 < 300ms
    """
    service = CandidateSearchService(db)

    filters = CandidateSearchFilters(
        skills=["Python", "JavaScript"],
        experience_level=["mid", "senior"],
        min_years_experience=3,
        location="San Francisco",
        remote_only=False,
        page=1,
        limit=20,
    )

    # Run query 20 times
    for i in range(20):
        result, elapsed_ms = performance_tracker.track(
            lambda: service.search_candidates(filters)
        )

    stats = performance_tracker.get_stats()

    print(f"\nüìä Candidate Search Performance:")
    print(f"   Runs: {stats['count']}")
    print(f"   Mean: {stats['mean']:.2f}ms")
    print(f"   p95: {stats['p95']:.2f}ms")
    print(f"   p99: {stats['p99']:.2f}ms")

    assert stats['p95'] < TARGET_P95_MS, \
        f"p95 ({stats['p95']:.2f}ms) exceeds target ({TARGET_P95_MS}ms)"

    print(f"‚úÖ p95 ({stats['p95']:.2f}ms) meets target (<{TARGET_P95_MS}ms)")


def test_application_timeline_performance(db: Session, performance_test_data, performance_tracker):
    """
    Test application timeline query (recent applications).

    Uses idx_applications_created_at index.
    """
    user_id = performance_test_data["users"][0].id

    # Run query 20 times
    for i in range(20):
        result, elapsed_ms = performance_tracker.track(
            lambda: db.query(Application)
            .filter(Application.user_id == user_id)
            .order_by(Application.created_at.desc())
            .limit(100)
            .all()
        )

    stats = performance_tracker.get_stats()

    print(f"\nüìä Application Timeline Performance:")
    print(f"   Mean: {stats['mean']:.2f}ms")
    print(f"   p95: {stats['p95']:.2f}ms")

    assert stats['p95'] < TARGET_P95_MS
    print(f"‚úÖ p95 ({stats['p95']:.2f}ms) meets target")


# ===========================================================================
# N+1 Query Prevention Tests
# ===========================================================================


def test_no_n_plus_1_in_application_listing(db: Session, performance_test_data):
    """
    Verify application listing doesn't trigger N+1 queries.

    Should use joinedload to fetch user and job in single query.
    """
    from sqlalchemy import event
    from sqlalchemy.engine import Engine

    query_count = []

    def count_queries(conn, cursor, statement, parameters, context, executemany):
        query_count.append(1)

    # Attach event listener
    event.listen(Engine, "before_cursor_execute", count_queries)

    try:
        job_id = performance_test_data["jobs"][0].id
        service = ApplicationService(db)

        applications, total = service.get_applications_for_job(
            job_id=job_id,
            page=1,
            limit=20,
        )

        # Access related data (should not trigger additional queries)
        for app in applications:
            _ = app.user.email
            _ = app.job.title

        # Should be ‚â§3 queries: 1 for applications+joins, 1 for count, maybe 1 for transaction
        assert len(query_count) <= 5, \
            f"Too many queries ({len(query_count)}) - possible N+1 problem"

        print(f"‚úÖ No N+1 queries detected ({len(query_count)} queries total)")

    finally:
        event.remove(Engine, "before_cursor_execute", count_queries)


# ===========================================================================
# Pagination Tests
# ===========================================================================


def test_pagination_limits_results(db: Session, performance_test_data):
    """
    Verify pagination properly limits results.
    """
    job_id = performance_test_data["jobs"][0].id
    service = ApplicationService(db)

    # Request page 1 with limit 20
    applications, total = service.get_applications_for_job(
        job_id=job_id,
        page=1,
        limit=20,
    )

    # Should return exactly 20 (or less if total < 20)
    assert len(applications) <= 20, "Pagination not working"
    assert total >= len(applications), "Total count incorrect"

    print(f"‚úÖ Pagination working correctly (returned {len(applications)}/{total})")


def test_pagination_performance_consistent(db: Session, performance_test_data, performance_tracker):
    """
    Verify pagination performance is consistent across pages.

    Page 1 and Page 10 should have similar performance.
    """
    job_id = performance_test_data["jobs"][0].id
    service = ApplicationService(db)

    # Test page 1
    _, time_page_1 = performance_tracker.track(
        lambda: service.get_applications_for_job(job_id=job_id, page=1, limit=20)
    )

    # Test page 10
    _, time_page_10 = performance_tracker.track(
        lambda: service.get_applications_for_job(job_id=job_id, page=10, limit=20)
    )

    # Pages should have similar performance (within 2x)
    assert time_page_10 < time_page_1 * 2, \
        f"Page 10 significantly slower ({time_page_10:.2f}ms vs {time_page_1:.2f}ms)"

    print(f"‚úÖ Pagination performance consistent (page 1: {time_page_1:.2f}ms, page 10: {time_page_10:.2f}ms)")


# ===========================================================================
# Slow Query Detection
# ===========================================================================


def test_no_queries_exceed_slow_threshold(db: Session, performance_test_data):
    """
    Verify no queries exceed slow query threshold (500ms).
    """
    from sqlalchemy import event
    from sqlalchemy.engine import Engine
    import time

    slow_queries = []

    def track_slow_queries(conn, cursor, statement, parameters, context, executemany):
        context._query_start_time = time.perf_counter()

    def detect_slow_queries(conn, cursor, statement, parameters, context, executemany):
        if hasattr(context, '_query_start_time'):
            elapsed_ms = (time.perf_counter() - context._query_start_time) * 1000
            if elapsed_ms > SLOW_QUERY_THRESHOLD_MS:
                slow_queries.append({
                    'statement': statement,
                    'time_ms': elapsed_ms,
                })

    event.listen(Engine, "before_cursor_execute", track_slow_queries)
    event.listen(Engine, "after_cursor_execute", detect_slow_queries)

    try:
        # Run various queries
        job_id = performance_test_data["jobs"][0].id
        service = ApplicationService(db)

        service.get_applications_for_job(job_id=job_id, page=1, limit=20)

        search_service = CandidateSearchService(db)
        search_service.search_candidates(CandidateSearchFilters(page=1, limit=20))

        # Verify no slow queries
        if slow_queries:
            for slow_q in slow_queries:
                print(f"\n‚ùå SLOW QUERY ({slow_q['time_ms']:.2f}ms):")
                print(f"   {slow_q['statement'][:200]}")

        assert len(slow_queries) == 0, \
            f"Found {len(slow_queries)} slow queries exceeding {SLOW_QUERY_THRESHOLD_MS}ms"

        print(f"‚úÖ No slow queries detected (all queries <{SLOW_QUERY_THRESHOLD_MS}ms)")

    finally:
        event.remove(Engine, "before_cursor_execute", track_slow_queries)
        event.remove(Engine, "after_cursor_execute", detect_slow_queries)


# ===========================================================================
# Benchmark Summary
# ===========================================================================


def test_performance_summary(db: Session, performance_test_data):
    """
    Generate comprehensive performance summary report.
    """
    print("\n" + "="*80)
    print("üìä PERFORMANCE TEST SUMMARY - Issue #66")
    print("="*80)

    print(f"\nTest Data Scale:")
    print(f"  - Companies: {NUM_COMPANIES}")
    print(f"  - Jobs: {NUM_JOBS}")
    print(f"  - Candidates: {NUM_CANDIDATES}")
    print(f"  - Applications: {NUM_APPLICATIONS}")

    print(f"\nPerformance Targets:")
    print(f"  - p95 Query Time: <{TARGET_P95_MS}ms")
    print(f"  - p99 Query Time: <{TARGET_P99_MS}ms")
    print(f"  - Slow Query Threshold: {SLOW_QUERY_THRESHOLD_MS}ms")

    print(f"\nIndex Coverage:")
    print(f"  - Applications: 5 indexes")
    print(f"  - Jobs: 4 indexes")
    print(f"  - Candidate Profiles: 5 indexes")
    print(f"  - Total: 32 indexes across 12 tables")

    print("\n" + "="*80)
    print("‚úÖ All performance tests passed!")
    print("="*80 + "\n")
